{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf05c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luminary/Documents/L3/Stage/tuto_fl/stage/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-03 10:01:31,030\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Flower 1.17.0 / PyTorch 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import Strategy\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from typing import Union\n",
    "from flwr.server.client_proxy import ClientProxy  # Correctly import ClientProxy\n",
    "from flwr.common import FitRes, Parameters\n",
    "import glob\n",
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5058fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import NDArrays\n",
    "MAX_ROUND = 1\n",
    "NUM_CLIENTS = 10 # nombre de client participant\n",
    "NUM_PARTITIONS = NUM_CLIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9934698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_datasets(partition_id, num_partitions: int):\n",
    "    # Définir les chemins d'accès\n",
    "    base_dir = f\"utkface_partitions/partition_{partition_id}\"\n",
    "    train_dir = os.path.join(base_dir, \"train\")\n",
    "    test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "    # Transformations pour images RGB (UTKFace est en couleur)\n",
    "    pytorch_transforms = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # pour images RGB\n",
    "    ])\n",
    "\n",
    "    # Charger les datasets\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=pytorch_transforms)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=pytorch_transforms)\n",
    "\n",
    "    # Créer des DataLoaders\n",
    "    trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(test_dataset, batch_size=32)  # ou un vrai split validation/test si disponible\n",
    "    testloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    return trainloader, valloader, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae873010-6a72-409b-a65b-38b85106e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class UTKFaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = [img for img in os.listdir(root_dir) if img.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        # Format: [age]_[gender]_[race]_[date&time].jpg\n",
    "        age, gender, race, _ = img_name.split('_')\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = int(age)  # tu peux changer pour `int(gender)` ou `int(race)`\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b4d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_channel=3, num_classes=117):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128*6*6, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    print(\"net state_dict keys:\", net.state_dict().keys())  # Imprime les clés\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images,labels in trainloader:\n",
    "            print(f\"Input shape: {images.shape}\")\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images,labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea070b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3bcbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3785ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cdce9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: list[tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: list[Union[tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> tuple[Optional[Parameters], dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate model weights using weighted average and store checkpoint\"\"\"\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(\n",
    "            server_round, results, failures\n",
    "        )\n",
    "\n",
    "        if server_round == MAX_ROUND:\n",
    "\n",
    "            if aggregated_parameters is not None:\n",
    "                print(f\"Saving round {server_round} aggregated_parameters...\")\n",
    "\n",
    "                # Convert `Parameters` to `list[np.ndarray]`\n",
    "                aggregated_ndarrays: list[np.ndarray] = fl.common.parameters_to_ndarrays(\n",
    "                    aggregated_parameters\n",
    "                )\n",
    "\n",
    "\n",
    "                # Convert `list[np.ndarray]` to PyTorch `state_dict`\n",
    "                params_dict = zip(net.state_dict().keys(), aggregated_ndarrays)\n",
    "                state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "                net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "                # Save the model to disk\n",
    "                torch.save(net.state_dict(), f\"model_round_{server_round}.pth\")\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93457c81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_PARTITIONS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DEVICE.type == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     32\u001b[39m     backend_config = {\u001b[33m\"\u001b[39m\u001b[33mclient_resources\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mnum_gpus\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m}}\n\u001b[32m     35\u001b[39m run_simulation(\n\u001b[32m     36\u001b[39m     server_app=server,\n\u001b[32m     37\u001b[39m     client_app=client,\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     num_supernodes=\u001b[43mNUM_PARTITIONS\u001b[49m,\n\u001b[32m     39\u001b[39m     backend_config=backend_config,\n\u001b[32m     40\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'NUM_PARTITIONS' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: NDArrays,\n",
    "    config: Dict[str, Scalar],\n",
    ") -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "    net = Net().to(DEVICE)\n",
    "    _, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(net, testloader)\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "def server_fn(context):\n",
    "    strategy = SaveModelStrategy(\n",
    "        fraction_fit=1.0,  # Utiliser 100% des clients pour l'entraînement\n",
    "        fraction_evaluate=1.0,  # Utiliser 10% des clients pour l'évaluation\n",
    "        min_fit_clients=1,  # Minimum de 10 clients pour l'entraînement\n",
    "        min_evaluate_clients=1,  # Minimum de 5 clients pour l'évaluation\n",
    "        min_available_clients=1,\n",
    "        evaluate_fn=evaluate\n",
    "    )\n",
    "    \n",
    "    config = ServerConfig(num_rounds=MAX_ROUND)\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "app = ServerApp(server_fn=server_fn)\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "backend_config = {\"client_resources\": None}\n",
    "if DEVICE.type == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_gpus\": 1}}\n",
    "\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec02532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc65f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bb85993",
   "metadata": {},
   "source": [
    "# Adapter le dataset en fonction du nombre de client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a1d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de fichiers trouvés : 24106\n",
      "✅ 15 partitions créées avec succès avec structure train/test et labels 0/1.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dossiers d'entrée\n",
    "input_dirs = [\"./dataset/UTKFace/part1\", \"./dataset/UTKFace/part2\", \"./dataset/UTKFace/part3\"]\n",
    "all_files = []\n",
    "\n",
    "# Collecte de tous les fichiers .jpg\n",
    "for part in input_dirs:\n",
    "    for file in os.listdir(part):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            all_files.append(os.path.join(part, file))\n",
    "\n",
    "print(f\"Nombre total de fichiers trouvés : {len(all_files)}\")\n",
    "\n",
    "# Mélange aléatoire des fichiers\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# Nombre de partitions\n",
    "num_partitions = NUM_PARTITIONS\n",
    "\n",
    "# Taille approximative d'une partition\n",
    "partition_size = len(all_files) // num_partitions\n",
    "\n",
    "# Dossier de base pour les partitions\n",
    "output_base = \"./utkface_partitions\"\n",
    "\n",
    "# Nettoyer puis recréer le dossier de sortie s'il existe\n",
    "if os.path.exists(output_base):\n",
    "    shutil.rmtree(output_base)\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "# Fonction pour copier les fichiers en fonction du split et du genre\n",
    "def process_files_partition(partition_files, partition_idx):\n",
    "    partition_dir = os.path.join(output_base, f\"partition_{partition_idx}\")\n",
    "    \n",
    "    # split interne en 80% train, 20% test\n",
    "    train_files, test_files = train_test_split(partition_files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for split, files in [(\"train\", train_files), (\"test\", test_files)]:\n",
    "        for filepath in files:\n",
    "            filename = os.path.basename(filepath)\n",
    "            try:\n",
    "                gender = filename.split(\"_\")[1]\n",
    "                if gender not in [\"0\", \"1\"]:\n",
    "                    continue  # ignore fichier étrange\n",
    "                target_dir = os.path.join(partition_dir, split, gender)\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                shutil.copy(filepath, os.path.join(target_dir, filename))\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur sur {filename} : {e}\")\n",
    "\n",
    "# Répartition des fichiers dans les partitions\n",
    "for i in range(num_partitions):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = (i + 1) * partition_size if i != num_partitions - 1 else len(all_files)\n",
    "    \n",
    "    partition_files = all_files[start_idx:end_idx]\n",
    "    process_files_partition(partition_files, i)\n",
    "\n",
    "print(\"✅ 15 partitions créées avec succès avec structure train/test et labels 0/1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32499ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
